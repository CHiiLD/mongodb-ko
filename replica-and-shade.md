# 몽고디비의 복제
## 복제 시스템
### Master-Slave
MongoDB의 복제 정책은 기본적으로 __Master-Slave__ 방식을 채택하고 있으며, master가 죽더라도 slave들 중에서 master를 선출할 수 있는 master 선출 방법을 채택하고 있다. 즉, Master가 죽으면, slave들이 투표를 진행하고 투표된 결과에 따라 새로운 master가 선출된다.

![그림 2-1](./images/pic2-1.png)

[그림 2-1]은 MongoDB의 복제 정책을 보여준다. MongoDB의 __master는 쓰기 연산__ 을 담당한다. 즉, 일반 Master-Slave 방식과 동일하게 쓰기는 master에서만 이루어진다. 이때 MongoDB는 쓰기 연산을 데이터 저장소와 Oplog라는 두 군데 영역에 저장한다. 데이터 저장소에는 B+ 트리로 구성된 데이터 저장소를 말하는 것으로, 쓰기 연산을 수행한 결과를 저장한다. 반면 __Oplog는 데이터 저장소에 저장된 데이터와는 달리 연산 수행과 관련된 명령 자체를 타임스탬프와 같이 저장한다__.  

### oplog
MongoDB의 slave는 아주 빠른 주기적으로 master에게 자신의 optime보다 큰 oplog를 달라고 요청한다. Slave의 요청은 master에 oplog 데이터를 요청할 때, 질의 요청 옵션을 QueryOption_AwaitData으로 보낸다. QueryOption_AwaitData는 대기하는 일정 시간 안에 응답할 데이터가 존재한다면 바로 응답하고, 응답할 데이터가 없다면, 일정 시간을 대기한다는 것을 의미한다. Oplog 질의에 대한 대기 시간은 5초이다. 즉, 5초안에 master에서 쓰기 연산이 발생하면 바로 데이터를 응답해 주고, 5초안에 쓰기 연산이 발생하지 않는다면, 데이터가 존재하지 않는다는 응답을 보내준다. Slave는 요구한 Oplog의 데이터가 존재하면 자신의 Oplog에 데이터를 저장한 다음에 바로 mater에 다시 Oplog 질의를 수행한다.  

### 동기화 처리
Slave의 동기화 처리는 쓰레드 한 개가 지속적으로 담당하고 있다. 만약, master와의 연결이 단절되어 Oplog를 동기화 시키지 못하게 되는 경우는 자신의 Oplog의 마지막 연산 시간을 저장하고, Oplog를 비우고 난 다음에 메모리에 보관된 모든 데이터를 데이터 저장소에 저장시킨다. 그리고, 5초 이후에 다시 master와의 연결을 시도한다.  


Master 역시 slave와의 동기화를 위해 한 개의 쓰레드를 만들어 slave와의 통신을 담당하고 slave가 요구하는 데이터를 전달한다. 따라서 master는 능동적 동기화 보다는 수동적 동기화 입장이므로, slave들의 요구에 따라 데이터를 전송하는 부담만 지게 된다. 이러한 MongoDB의 철학은 master의 역할을 쓰기 연산에 집중 하도록 구성하여 빠른 속도를 구사할 수 있도록 만들고 있다. 실제 성능 테스트를 수행하였을 경우에도, 복제를 설정한 상태에서 master의 부하가 가중되지는 않는다. 성능 이슈는 5% 정도로 복제가 전체 시스템의 성능 여하를 결정하지 않는다. 다만, 복제를 위한 slave의 개수를 많이 둘 경우에는 master의 동기화 요구가 많아지게 되므로, 그 만큼의 속도 저하는 고려하여야 한다.

## 복제 동기화와 마스터 선출
### heartbeat

![그림 2-2](./images/pic2-2.png)

[그림 2-2]의 MongoDB는 한 개의 master와 두 개의 slave로 구성된 복제 집합replica set을[1]구성하고 있다. MongoDB는 복제 집합으로 구성된 각각의 노드는 자신을 제외한 다른 노드들이 죽었는지 살았는지를 검사하기 위해 [그림 2-2]과 같은 __heartbeat__ 를 사용한다. MongoDB의 heartbeat는 2초 단위로 수행되며, heartbeat을 받은 서버는 자신의 상태 코드를 heartbeat을 요청한 서버에 보내준다. [표 2]는 heartbeat을 통해 전달된 서버 상태 코드를 보여준다.

|상태 코드|	내용|
|-------|---|
|RS_STARTUP|	서버가 시작 중이거나 또는 복제 집합의 초기화를 수행하는 상태|
|RS_STARTUP2|	초기화를 위한 복제 환경 로드는 완료하였지만, primary[2]를   선출하는 상태|
|RS_PRIMARY|	서버 자신이 primary 상태|
|RS_SECONDARY|	서버 자신이 secondary[3] 상태|
|RS_RECOVERING|	서버가 secondary로 상태가 변경되기 전에 primary로부터 복제 동기화를 수행하는 상태
|RS_ROLLBACK|	서버가 secondary로 상태가 변경되기 전에 primary로부터 복제 동기화를 수행하는 상태로,  서버가 primary보다 더 많은 데이터를 가지고 있어서 primary 상태로 저장된 데이터로 되돌리는 상태|
|RS_FATAL|	서버가 복제 집합 안에서 네트워크 단절과 같이 완전한 offline 상태는 아니지만, 심각한 문제가 발생한 상태|
|RS_SHUNNED|	서버가 어떠한 복제 집합에도 속하지 않은 상태|

### 마스터 선출
Master 서버의 heartbeat는 항상 복제 집합을 구성하고 있는 노드 개수의 과반수만큼을 유지하고 있어야 한다. 만약 master 서버가 과반수의 heartbeat을 가지고 있지 않다면, 서버는 slave로 변환되며, 복제 집합은 master 부재에 따른 투표를 시행한다. MongoDB의 master 선출과정은 다른 Google의 PAXSOS와 유사하다. 단지 차이점이 있다면, master가 될 수 있는 자격 조건에 대한 옵션 priority와 votes를 가지고 있다는 차이점이 있다. __Priority는 자신이 master가 될 수 있는 우선 순위를 나타내는 것으로 priority가 높은 서버가 master가 될 가능성이 높다.__ 만약 priority가 0이라면 이는 자신 자신은 master로 설정할 수 없음을 나타낸다. 또한 __votes는 투표할 수 있는 개수를 의미하는 것으로, master는 자신을 포함한 복제 집합의 노드 개수의 과반수 투표를 가져야 한다.__ 그렇다면, votes와 priority와의 차이점은 무엇인가? Votes는 투표권을 말한다. 즉, 한 복제 집합에서 보유한 투표권은 각각의 노드가 가지고 있는 투표권을 모두 더한 것을 말한다. 아무런 값도 설정하지 않았다면 투표권은 한 노드 당 한 개의 값을 가질 것이다. 예를 들어 보자. 총 4개의 노드로 구성된 복제 집합이 [그림 2-3]과 같이 구성되었다고 가정하자.

![그림 2-3](./images/pic2-3.png)


[그림 2-3]의 (a)는 정상적인 상태의 복제 집합으로 총 투표권은 6개로 설정되어 있다. 그런데, (b)번과 같이 가장 많은 투표권을 가지고 있는 노드가 죽었다면, 갑자기 3개의 투표권이 없어진다. 그러면 [그림 2-3]의 복제 집합은 기존에 존재하던 6개의 투표권 중에서 살아있는 다른 노드들의 투표권을 더해도 과반수를 넘지 못한다. 따라서, (b)와 같은 시스템에서는 master를 선출할 수 없다. MongoDB의 votes가 이해되었다면, priority와의 차이점도 정확한 의미를 알 수 있다. Votes는 가장 중요한 한 개의 노드를 기준으로 해당 노드가 죽었을 경우에 전체 시스템을 read-only로 만들 수 있고, priority는 master의 가중치를 두어서, 언제든지 master 선출과정에서 master가 될 확률을 높인다는 것이다. 두 개가 같은 의미지만, 약간 다른 의미를 가지고 있음을 주의하자.

#### 마스터의 자격 조건
+ 복제 집합에서 과반수 노드들과 연결을 유지하는 있어야 한다.
+ Priority가 0보다 커야 한다.
+ 서버가 유지하고 있는 optime이 연결될 수 있는 다른 노드들의 optime 중에서 가장 최신 opime을 기준으로 10초 이내에 있어야 한다.

#### 투표 과정
1. 자신의 master가 될 수 있는 서버인지 판단하고, master가 될 수 있는 서버는 다음과 같은 일을 수행한다.
1. 자신이 master임을 인지하고 아주 짧은 시간 동안 기다린다.
1. 자신이 master임을 다른 노드에 통보한다.
1. 만약, 다른 노드들로부터 전달받은 메시지가 있다면, 자신의 priority와 비교하여 낮거나 또는 자신의 optime 보다 최신의 상태가 아니면 거부권(NO)을 발동한다.
1. 만약, 거부권을 행사했다면, 자신의 master가 될 수 있다는 것이므로, 거부권을 행사한 서버는 YES를 과반수 이상 받아야만 한다.
1. 만약, 2)번에서 거부권을 행상하지 않은 서버는 자신이 master의 자격이 없다는 것을 의미하는 것으로, 1)번에서 발송한 메시지에 대해 1분안에 거부권을 모두 받아야만 하며, 거부권을 모두 수신한 서버는 투표를 완료하게 된다.[6]


위의 절차에서 4)번의 1분은 최대 대기 시간을 의미하는 것으로, 대부분 네트워크 부하 또는 서버의 부하가 발생하지 않는다면, 10초안에 투표는 완료되어 master가 선출된다. 초기 시스템이 로딩하였을 때의 master 선출 시간은 약 5초 정도이다. MongoDB의 master 선출 과정에서 priority에 의해 가장 최신의 업데이트 정보를 가지고 있지 않은 서버가 master로 선출될 수 있다. 만약 이러한 경우에 __복제 동기화 과정에서 자신의 optime과 master의 optime을 비교하여 master보다 최신 데이터는 버리게 된다.__ 이와 같이 버려진 데이터는 나중에 관리자를 통해 __수동 복구할 수 있도록 BSON 형태의 파일로 저장__ 된다. 이와 같은 경우는 master 선출을 강제로 특정 서버로 유지하기 위해 사용될 경우에 발생되며, master 후보군에서 보여지듯이 10초 이내의 데이터만 유실 가능성이 발생된다.

## 복제의 한계
>한 복제 집합을 구성할 수 있는 노드의 최대 개수는 12개이다.  
>한 복제 집합에서 투표할 수 있는 노드의 최대 개수는 7개이다.

상기와 노드의 개수에 제한을 둔 것은 성능과 관련이 있다. 즉 Slave 개수의 제한은 master의 부하를 결정하고 투표 노드의 제한은 master 선출의 최대 시간과 관련 있다. 아마 10gen이 실험적으로 위와 같은 한계사항을 결정한 것이 아닌가 판단된다.

## 저널링 시스템
MongoDB는 데이터 손실을 최소화하기 위한 방법으로 저널링을 제공한다. 저널링이란 일종의 로그와 같은 것으로 MongoDB에 __데이터의 변화에 따른 모든 연산에 대해 로그를 적재__ 한다. 저널링 시스템은 [그림 2-4]와 같은 구조로 구성된다.

![그림 2-4](./images/pic2-4.png)

[그림 2-4]의 (a)는 일반 데이터에 대한 저널링을 의미하고, (b)는 인덱스 데이터에 대한 저널링을 의미한다. Oplog는 데이터 연산 중에 쓰기, 수정, 삭제와 같이 데이터 변동이 발생될 때의 연산 자체를 저장하여 master와 slave간의 동기화를 수행하는 high-level transaction을 보장하는 저장소인 반면, 저널링은 mongod 한 노드에 개별적으로 설정하여 저장되는 low-level 로그와 같은 구성이다.

MongoDB는 __충돌에 의해, 복구를 수행하고자 한다면, 우선 저널링을 통해 개별 mongod들의 복구를 수행한다. 저널 파일에 저장된 데이터는 [그림 2-4]와 같이 일반 데이터 저장소의 데이터와 Oplog의 데이터를 모두 저장하고 있어서, 일반 데이터 저장소만 복구하는 기능 외에 복제를 위한 Oplog의 동기화 데이터까지 복구가 가능하다.__

데이터 중복성에 대해서 논의해 보자. [그림 2-4] (a)와 같이 복제와 저널링을 모두 사용할 경우에는 한 개의 데이터에 대해서 총 4X의 중복성을 가지고, (b)와 같은 인덱스에서는 2X의 중복성을 가진다. 데이터와 인덱스의 사용비율에 대한 일반적 통계 비율에 의하면 데이터 중복성은 총 2.5X 정도 사용된다.

저널링은 데이터 중복성을 가지기 때문에, 시스템 전체 속도와도 관련 있다. 아무래도 저널링을 사용하면 속도는 떨어진다. 이러한 문제를 위해 MongoDB는 group commits를 제공한다. Group commits란 저널링 로그가 발생할 때 마다, 한번씩 데이터를 파일에 저장하는 것이 아니라, 일정 시간 동안 발생된 데이터를 한 번에 저장하는 방식을 말한다. 디폴트 값은 100ms 로 설정되어 있으며, 관리자에 의해 기동할 때 2 ~ 300ms의 범위의 값을 설정할 수 있다.

## 데이터의 유실 가능성

앞 절에서 살펴보았듯이 MongoDB의 데이터 유실 가능성이 존재하지 않는 것은 아니다. 복제의 경우는 slave가 아무리 빨리 데이터 동기화를 한다고 하여도, master와의 통신 지연 시간만큼의 차이를 가질 수 있고, 저널링 역시 group commits의 시간에 의해 차이가 발생된다.

하지만, slave의 Oplog 동기화는 시스템 부하가 없는 경우라면, 쓰레드에 의해 아주 빠르게 주기적으로 동기화를 수행하기 때문에, 쓰기 연산이 엄청난 부하를 가지는 않는 상태에서는 Oplog의 동기화로 문제시 되지 않는다. 다만, 부하를 견디지 못해 서버가 죽는 경우라면, Oplog에 동기화 되지 않은 채 남아있는 데이터 연산을 잃어버리는 현상이 나타난다.

저널링은 데이터 저장소에 문제가 발생하여 데이터가 유실되었을 경우에, 복구를 수행하는 것이다. 복구는 MongoDB를 재 기동시킬 때 또는 복구 명령을 직접 수행하였을 경우에 이루어진다. __저널링__ 파일에 데이터를 저장하는 방법 역시, 앞 절에서 살펴보았듯이 디폴트 값이 100ms의 오차를 가지고 있다. __만약 100ms 안에 데이터가 존재하여 시스템이 죽어서 메모리에 저장된 데이터가 날라갔다면, 이는 복구가 불가능하다.__

이러한 경우는, 복제로 설정된 상태에서 master와의 Oplog를 통한 동기화를 수행할 수 있으므로, 일정 부분 복구가 가능하다. 하지만, master 자체가 죽었다면 복구가 불가능한 데이터가 일부 남아 있을 수 있다.(Oplog의 동기화는 slave가 master에 요청하는 것이기 때문에, slave는 master의 데이터보다 같거나 적은 데이터를 유지한다.) MongoDB는 이러한 문제점을 해결하기 위해 일관성 정책에서 복제의 Oplog와 관련된 REPLICAS_SAFE, MAJORITY를 제공하고, 저널링과 관련된 JOURNAL_SAFE를 제공한다. __REPLICAS_SAFE__ 는 master를 제외한 적어도 한 개의 slave의 Oplog가 동기화가 완료된 상태를 의미하고, __MAJORITY__ 는 임의의 복제 집합으로 구성된 노드들의 과반수가 Oplog 동기화에 성공하였음을 의미한다. 예를 들어, 복제 레벨을 5로 구성하였다면(Master 1대와 slave 4대로 구성), master를 포함한 3대가 동기화 되었음을 알려준다. __JOURNAL_SAFE__ 는 저널 파일에 로그가 저장될 때까지 쓰기 연산을 기다리는 것으로, 만약 group commits가 100ms로 설정되어 있다면, 한 번의 쓰기 연산은 최대 100ms를 기다릴 수 있음을 나타낸다.

이외에도 __FSYNC_SAFE__ 모드가 있는데, 이는 MongoDB가 메모리에 있는 데이터를 물리적 저장소에 저장시킬 때까지 쓰기 연산을 기다리는 것을 의미한다. MongoDB의 fsync는 디폴트 값이 60초로 설정되어 있다. 따라서 한 번의 쓰기 연산은 최대 60초를 기다릴 수 있다.

MongoDB의 데이터 유실과 관련된 사항은 일반 데이터베이스에서 고려될 수 있는 일반적인 사항이다. MongoDB 역시 이러한 부분을 옵션을 이용하여 처리할 수 있도록 제공하고 있고, 또는 쓰기 연산에 WriteConcern을 조정하여 특정 연산에 대한 트랜잭션을 보장할 수 있도록 API를 제공하고 있다.

>일관성과 데이터 유실 문제가 심각한 트랜잭션 보장 데이터에 대해서는 복제로 구성된 시스템에서는 REPLICAS_SAFE로 처리하는 것이 바람직하며, standalone으로 사용하는 시스템에서는 저널링의 group commits 시간을 줄이고, JOURNAL_SAFE를 사용하는 것이 바람직하다. 물론 이러한 두 가지 방법 모두 성능과 밀접한 관련이 있으므로, 프로그래머에 의한 적절한 조절이 필요하다.

>마지막으로, 복제가 이유 없이 수행되지 않는다는 것은, 첫째 시스템 부하가 너무 많아서 slave의 동작이 의심스러운 경우, 또는 master의 fail에 대해서 master 선출과정에서 slave들이 master 정보를 갱신하지 못해, Oplog의 동기화를 수행하지 못하는 경우이다.

>전자의 경우는 한 노드에 여러 개의 slave를 설정하는 경우이거나 다른 프로세스의 동작이 MongoDB의 slave가 원활하게 동작할 수 없도록 막는 경우이다. 가급적 MongoDB의 coordinate하는 과정에서 slave를 중복하여 설치하는 것은 복제 동기화에 부담을 줄 수 있으므로, 피하는 것이 바람직하다. 후자의 경우는 예전 버전에서 이러한 문제점들이 발견되어 master를 읽는 경우가 있었으나, 현재는 그러한 버그는 모두 수정된 것으로 알고 있다. 만약 slave를 arbiter와 같이 사용할 경우에, arbiter는 master 선출을 위한 투표 이외에 아무런 작업을 하지 않으므로, 복제와는 관련이 없다.

>완벽한 프로그램은 존재하지 않는다. 어떤 이유에서라도 프로그램 오동작으로 인해, 프로세스가 충돌이 날 수 있다. 이러한 경우를 대비하기 위해, 중요한 시스템에서는 복제 레벨을 올려두거나 또는 일관성 정책을 조절하는 등 여러 가지 대비하여 시스템을 구축하는 것이 가장 좋은 fail-over 방법이다.

***
# 몽고디비의 락
C++로 개발된 MongoDB는 boost의 shared_mutex를 이용하여 락 시스템을 개발하였다. shared_mutex는 기본적으로 one writer many reader의 구조를 가지기 때문에 __MongoDB는 쓰기 락이 한 개만 존재__ 한다. 이러한 점은 MongoDB의 쓰기 연산에서 부하가 발생할 경우, 전체적인 시스템 부하를 야기할 수 있다.  하지만, 10gen에서는 쓰기 연산 속도가 매우 빠르기 때문에, MongoDB의 락이 부하를 발생시키지 않는다고 말한다. 사실 NoSQL은 트랜잭션이 없기 때문에 쓰기 연산에 대한 부하가 RDB 처럼 심하지 않다. 즉 __근본적인 쓰기만 수행하면 되기 때문에 매우 빠른 특징을 가진다. 또한 MongoDB는 1차적으로 메모리에 데이터를 쓰기 때문에 쓰기 연산 속도가 빠르다는 것이 보장된다.__

그렇다고, MongoDB의 락 시스템이 완전하다고 볼 수 없다. MongoDB는 쓰기 락을 시스템에 한 개를 유지하기 때문에, 아무래도 최대 쓰기 연산의 한계를 가진다. 이말은 클라이언트의 개수와는 상관 없이 최대 저장 속도가 락에 의해서 결정된다는 것이다. 이러한 문제점은 버전 2.2에서 컬랙션 별로 쓰기 락을 구현하여 어느정도 해소시켰다. 본 장에서는 이러한 MongoDB의 락 시스템에 대한 구조를 살펴보도록 한다.

## Boost의 shared_mutex
Boost에서는 일반 mutex의 기본 개념보다 좀 더 유연한 형태의 락을 설정할 수 있도록 __upgradable mutex__ 를 제공한다. 정확하게 표현하면, upgradable mutex는 mutex의 가능성을 판단하여 여러 개의 쓰레드가 동시에 락을 공유할 수 있도록 만든 기법이다. 이러한 기법을 __multi reader and single write__ 기술이라고 말한다. 읽기는 데이터 수정이 발생되지 않기 때문에 여러 개의 읽기 연산이 수행되어도 공유하는 데이터에 영향을 주지 않는다. 반면, 쓰기 연산은 공유하고 있는 데이터 수정이 발생하기 때문에 다른 쓰레드에서 읽기 또는 쓰기 연산과 충돌되어 데이터 일관성 문제가 발생한다. __즉 쓰기 연산은 배타적으로 락이 설정되어야 한다.__
Upgradable mutex는 이와 같이 읽기 연산은 쓰기 연산이 없는 동안, 아무런 제재 없이 읽기가 원활하게 이루어져야 하고, 쓰기 연산은 다른 쓰레드들의 읽기 또는 쓰기 연산이 자신의 쓰기 연산이 완료될 때까지 사용할 수 없도록 만들어 준다.

쓰기 연산을 수행하기 위한 절차를 생각해 보자. 쓰기 연산은 공유 데이터에 접근하기 위해 현재 공유 데이터를 사용하고 있는 쓰레드들이 있는지 판단한다. 만약 공유 데이터를 사용하고 있는 쓰레드가 있다면, 해당 쓰레드가 사용을 완료할 때까지 기다려야 한다. 자신을 제외한 모든 쓰레드가 공유 데이터의 사용을 완료하였다면, 대기 중이던 쓰기 연산은 배타적 락을 설정하여 다른 쓰레드들이 자신의 작업이 완료될 때까지 공유 데이터에 접근할 수 없도록 만든다.

반대로, 읽기 연산은 현재 공유 데이터를 액세스하기 위해 설정된 락들을 우선 조사하여야 한다. 공유 데이터에 설정된 락이 모두 읽기 락이라면 읽기 연산은 바로 공유 데이터에 접근할 수 있다. 하지만, 다른 쓰레드가 설정한 락이 쓰기 연산을 위한 배타적 락이라면, 읽기 연산은 배타적 락이 해제될 때까지 기다려야 한다.

Upgrade mutex는 쓰기 연산과 읽기 연산을 위해 3가지 유형의 락을 제공한다.

### 배타적 락 exclusive lock
배타적 락은 일반 mutex와 비숫하다. 임의의 한 쓰레드가 배타적 락을 획득했다면, 다른 쓰레드들은 어떠한 락도 이미 획득된 배타적 락이 해제되지 않는다면 획득할 수 없다. 임의의 쓰레드가 공유 또는 업그레이드 락을 가지고 있다면, 배타적 락을 획득하려는 쓰레드는 블록된다. __배타적 락은 데이터를 수정하기 위한 쓰레드에서 사용한다.__

### 공유 락 Sharable lock
임의의 쓰레드가 공유 락을 획득하였다면, 다른 쓰레드들은 공유 또는 업그레이드 락을 획득할 수 있다. 만약 다른 쓰레드가 배타적 락을 가지고 있다면, 공유 락을 획득하기 위해 시도하는 쓰레드는 블록된다. __공유 락은 데이터를 읽기 위한 쓰레드에서 사용된다.__

### 업그레이드 락 Upgradable lock
임의의 쓰레드가 업그레이드 락을 획득하였다면, 다른 쓰레드들은 공유 락을 획득할 수 있다. 만약 다른 쓰레드가 배타적 또는 업그레이드 락을 가지고 있다면, 업그레이드 락을 획득하려는 쓰레드는 블록된다. 업그레이드 락은 업그레이드 락을 설정한 쓰레드는 공유 락을 획득한 다른 쓰레드들이 락을 모두 해제하면 자동으로 배타적 락을 획득할 수 있도록 보장해 준다. 업그레이드 락은 일반적으로 쓰레드들이 읽기를 수행하지만, 데이터를 수행하기를 원하는 쓰레드에서 사용할 수 있다. 즉, __쓰기를 수행하는 쓰레드는 업그레이드 락을 사용하고, 읽기를 수행하는 쓰레드는 공유 락을 사용한다.__ 한가지 주의할 점은 업그레이드 락은 배타적 락과 같이 오로지 한 개의 쓰레드에서만 획득할 수 있다.

## 몽고디비의 락 시스템
앞 절에서도 논하였지만, MongoDB는 boost의 shared_mutex를 기반으로 락 시스템을 만들었다. 따라서, __읽기는 동시에 여러 쓰레드에서 동시에 수행되지만, 쓰기는 한 개의 쓰레드에서만 수행된다.__ 이러한 락 시스템은 다음과 같은 특징을 가진다.

+ 읽기 락은 쓰기 락이 설정되지 획득되지 않은 이상 병렬로 처리되지만, 쓰기 락이 획득되면 대기 상태로 빠진다. 쓰기 락이 해제되면 읽기 연산이 수행된다.
+ 쓰기 락은 읽기 락이 모두 해제된 상태에서 획득되며, 배타적 락을 형성한다.

따라서, __MongoDB는 쓰기와 읽기가 동시에 수행되지 않는다.__ 만약 시스템이 읽기를 많이 시도할 경우에는 쓰기가 블록 되고, 쓰기를 많이 수행할 경우는 읽기가 블록 된다.

![그림 3-1](./images/pic3-1.png)
[그림 3-1]은 MongoDB를 복제 시스템으로 구성하고 클라이언트가 읽기 연산을 수행하는 상태를 보여주고 있는 것으로 Slave_OK를 이용하여 master와 slave 모두 읽기가 가능한 상태를 나타낸다. 따라서 MongoDB는 master와 slave 모두 읽기 연산을 요청한다.  

(a)는 master와 slave 모두 락이 설정된 것이 없기 때문에 바로 읽기 연산 결과를 리턴 한다. __MongoDB 클라이언트는 master와 slave 둘 중에 하나가 먼저 리턴 된 값을 받아 그 결과를 응용에 전달한다.__  

(b)의 경우는 master와 slave 모두 읽기 락이 획득되어 있지만, __읽기 락이 공유 락이기 때문에 (a)와 같이 바로 연산의 결과 값을 리턴 한다.__

![그림 3-2](./images/pic3-2.png)
[그림 3-2]는 [그림 3-1]의 확장된 형태로 master와 slave에 쓰기 락이 구성될 때를 보여준다.  

(a)는 master에 쓰기 락이 설정된 것으로 클라이언트가 보낸 읽기 연산이 블록 된다. 따라서, 클라이언트는 slave로부터 전달 받은 읽기 연산 결과를 먼저 받게 되고, 응용에 해당 결과를 통보한다. __속도는 slave의 읽기 연산 속도와 동일하며, 쓰기 락에 따른 지연은 발생되지 않는다.__   

(b)는 조금 다른 상황이다. Master와 slave 모두 쓰기 락이 획득한 상태이다. __이러한 경우는 master와 slave로 보낸 읽기 연산 모두가 블록 되기 때문에, 읽기 연산이 지연된다.__ Slave의 쓰기 락은 Oplog의 동기화 또는 slave의 인덱스 재생성(foreground indexing) 때 발생된다. 만약 동기화 데이터가 많을 경우는 master의 락이 먼저 해제되어 master로 부터 읽기 연산의 결과를 받아 들일 것이다. 일반적으로 쓰기 연산은 벌크 삽입(bulk insert)인 경우를 제외하고 장시간을 요하지는 않는다. 따라서, 일반 slave의 Oplog 동기화를 위한 쓰기 락 보다 master의 쓰기 락이 먼저 해제 되는 것이 일반적이다.

## 몽고디비의 Global Lock
MongoDB의 global lock은 용어에 풍겨 나오는 이미지에 의해 여러 개의 노드로 구성된 MongoDB 시스템에서 유일하게 사용되는 전체 락 이라고 생각할 수 있지만, 이는 전체 락 이기 보다는 __MongoDB 시스템의 락 정보를 저장하고 있는 저장소이다.__ 저장하는 정보로는 현재 락 상태, 락 지속 여부, 현재 대기 중인 읽기 또는 쓰기 연산 수, 그리고 클라이언트 개수가 있다. [표 3]는 MongoDB의 serverStatus를 통해 취득할 수 있는 global lock의 항목을 보여준다.

[표 3] Global lock 항목

|항목|	내용|
|---|-------|
|globalLock.totalTime|	Global lock이 생성된 시간으로 시스템이 기동되고 난부터 수행중인 시간을 의미하다.(단위 : microseconds)|
|globalLock.lockTime|	Lock 지속시간을 나타내는 것으로, 프로세스가 기동하고 난 다음부터 현 시점까지 lock이 걸려있던 시간의 합을 의미한다.(단위 : microseconds)|
|globalLock.ratio|	lockTime / totalTime 의 값|
|globalLock.currentQueue.total|	currentQueue.readers + currentQueue.writers 의 값|
|globalLock.currentQueue.readers|	연산 중에서 lock이 걸린 읽기 연산의 개수|
|globalLock.currentQueue.writers|	연산 중에서 lock이 결린 쓰기 연산의 개수|
|globalLock.activeClients.total|	activeClients.readers + activeClients.writers 의 값|
|globalLock.activeClients.readers|	읽기 연산을 수행하고 있는 클라이언트의 개수|
|globalLock.activeClients.writers|	쓰기 연산을 수행하고 있는 클라이언트의 개수|

[표 3]의 값에서 주목하여야 할 값은 `lockTime`이다. `lockTime`은 MongoDB가 쓰기 락이 수행된 시간을 누적시킨 값으로, 값의 증가가 빨라지면 lockTime이 길어진다는 의미이다. 이 값이 길어지게 되는 원인은 여러 가지 발생할 수 있는데, 대부분 시스템 부하가 발생되어 lockTime이 길어진 것으로 보아야 한다. 한가지 분명한 사실은 쓰기 연산이 발생할 때마다 lockTime은 증가하며, __전체 시간 대비 lockTime이 증가하면 시스템 부하가 발생하였다고 예측할 수 있다.__

[표 3]의 `currentQueue`는 __MongoDB가 쓰기 락이 수행될 경우, 쓰기 락에 의해 대기 중인 연산을 의미한다.__ 명칭 큐로 설정되어 있어서 MongoDB가 큐에 연산을 넣고 해당 연산을 빼내어 처리한다고 판단할 수 있으나, 소스 코드상으로 볼 때, 연산을 요청한 클라이언트 리스트에서 해당 정보를 취득한다. 즉, 클라이언트가 요청할 때, 한 개의 쓰레드가 생성되는데, 이러한 쓰레드 리스트 중에서 현재 락이 걸린 연산들의 개수를 리턴 한다.

[표 3]의 `activeClients`는 __연산을 요청한 클라이언트의 개수를 저장__ 하는 것으로, 읽기 연산과 쓰기 연산을 요청한 클라이언트의 개수가 된다. 클라이언트의 한 개의 요청은 서버의 한 개의 쓰레드를 만들어 클라이언트의 요청이 할당된다. 서버에 쓰기 락이 활성화되지 않았다면, 서버가 생성한 쓰레드는 락이 걸리지 않는다. `cuurentQueue`와 `activeClients`는 모두 동일한 서버의 클라이언트 리스트에서 정보를 취득하지만, 임의의 클라이언트가 락이 걸려 있는지에 따라 데이터 값의 차이가 나타난다. 따라서, `currentQueue`의 값이 activeClients의 값 보다 클 수 없다.

MongoDB의 구성 요소인 mongos는 mongod와 달리 클라이언트의 역할을 담당하므로, `lockTime`, `currentQueue`, `activeClients`의 값이 나타나지 않는다. Global Lock은 서버 관점에서 클라이언트가 요청한 연산에 대한 락 상태를 보여주는 것이다. 따라서, [표 3]의 값은 mongod에서만 유효하다. 또한, [표 3]는 각각의 mongod에 따라 다른 값이 나타난다. MongoDB가 말하는 Global Lock은 한 개의 mongod 프로세스에서 발생하는 값을 의미한다. 만약 여러 개의 노드로 구성된 시스템에서의 Global Lock의 총합을 알고자 한다면, 각 mongod 프로세스에서 [표 3]의 globalLock 값을 취득한 뒤, 합쳐야 한다.

MongoDB는 메모리 데이터베이스의 성격을 가지고 있다. 따라서, 메모리에 데이터를 쓰는 시간 만큼의 쓰기 락이 주어진다. 이는 MongoDB의 일관성 정책이 NORMAL인 경우에 해당된다. 만약 일관성이 NORMAL이 아닌 SAFE라면 쓰기 락이 시간도 같이 올라간다.

***
# 몽고디비의 샤딩
MongoDB의 샤딩 구조는 분산데이터베이스의 전통적인 분할 3계층 구조 – 응용, 중개자, 데이터 계층 – 를 가진다. __중개자 역할을 담당하는 mongos(라우터)__ 와 __중개자의 메타 정보를 저장하고 있는  config 서버__, 그리고 __데이터를 저장하는 mongod__ 로 구성된다. 본 절에서는 이러한 MongoDB의 샤딩 구조에 대한 기본적인 개념부터, 각 모듈들이 처리하는 내부 구조에 대해서 알아본다.

## 샤딩 시스템 개요
데이터베이스에 저장하는 데이터의 규모가 대용량이 되면서, 기존에 사용하는 데이터베이스 시스템의 용량의 한계를 맞이하게 된다. 이러한 시점에서 관리자는 시스템 업그레이드를 고려한다. 흔히 말하는 시스템 업그레이드란 장비를 고 사양으로 전환하여 기존 시스템의 처리 용량을 증대시키는 것을 말한다. 이를 Scale-Up이라고 한다.

Scale-Up은 기존 사용하는 장비의 업그레이드이기 때문에, 기존 장비 사양에 따라 증설 계획이 결정된다. 하지만, 일반적으로 데이터베이스 장비는 구매 당시, 시스템 안정성을 고려하여 가장 우수한 성능의 시스템으로 구매한다. 이러한 장비를 다시 고 사양으로 업그레이드한다는 것은 장비 하나를 더 추가하는 것보다 비싼 대가를 치르게 된다.

그렇다면, 비용대비 기존 장비를 어떻게 활용하는 것이 대용량 데이터베이스를 저장하는 방법일까? 이러한 문제점을 해결하기 위해 __데이터베이스 시스템은 소프트웨어적으로 데이터베이스를 분산시켜 처리할 수 있는 구조__ 를 만들었다. 이러한 기술 중 하나가 __샤딩(sharding)__ 이다. 샤딩은 데이터베이스가 저장하고 있는 테이블을 테이블 단위로 분리하는 방법과 테이블 자체를 분할하는 방법으로 나누어진다.

여기서, 우리가 기존에 사용하던 데이터베이스에 총 5개의 테이블이 저장되어 있다고 가정하자. 그리고, 5개의 테이블에 대용량 데이터가 몰려 시스템 사양을 초과하여, 이를 샤딩하기 위해 한 대의 테이터베이스 시스템을 2대로 증설한다고 가정하자. 이와 같을 경우, 전자의 경우인 테이블 단위로 분리하는 방법은 총 5개의 테이블에서 첫 번째 장비에는 테이블 3개, 두 번째 장비에는 테이블 2개로 분리하는 것을 말하고, 후자의 경우인 테이블 자체를 분할하는 방법은 총 5개의 테이블을 모두 반반씩 쪼개어 두 개의 장비에 나누어 저장하는 방법을 말한다. [그림 4-1]의 (a)는 테이블 단위로 분리하는 방법을 (b)는 테이블 자체를 분할하는 방법에 대해서 보여준다.

![그림 4-1](./images/pic4-1.png)

[그림 4-1]과 같이 한 곳에 저장되어있던 테이블을 두 개 이상의 샤드(shard, 개별적 노드를 샤딩에서는 샤드라고 말한다.)로 테이터를 분리하는 경우, 테이터베이스 시스템은 사용자 질의에 따라 검색되어야 할 데이터가 어느 샤드에 있는지 위치 정보를 관리하는 모듈이 필요하다. 이러한 __샤딩된 데이터의 위치 정보를 관리하는 모듈을 중개자broker라 하며, 질의를 분석하여 샤드를 선택하고, 응답에 대한 결과를 전달하는 역할을 담당한다.__ [그림 4-2]은 샤딩 시스템의 구조를 보여준다.

샤딩 시스템은 [그림 4-2]와 같이 크게 3 계층으로 – 응용계층, 중개자 계층, 데이터(서버 또는 샤드) 계층 – 으로 구성된다. 응용계층은 데이터에 접근하기 위해 중개자를 통해 모든 데이터 입출력을 진행한다. 따라서 응용은 샤딩된 데이터베이스 시스템의 자세한 구조를 알 필요 없이 마치 잘 추상화된 한 개의 데이터베이스가 존재하는 것처럼 응용 계층은 느끼게 된다. 데이터 계층은 여러 개의 샤드(또는 서버)들로 구성되며, 각각의 샤드는 일반 데이터베이스 시스템과 동일한 역할을 담당한다. 가운데 있는 중개자 계층은 샤딩 시스템의 가장 핵심적인 부분으로 샤드 메타 정보(shard meta data)를 저장하여 중개자로부터 전달된 질의를 분석하여 적절한 샤드에 명령을 수행하여 그 결과를 응용에 전달한다.

![그림 4-2](./images/pic4-2.png)

중개자 계층이 저장하는 샤드 메타 정보는 분할을 결정하는 정책에 따라 다음과 같이 분류된다.

+ 형태에 따른 분류  
형태에 따른 분류는 [그림 4-1]과 같이 테이블 자체가 분할되기 때문에 데이터 유형에 따라 저장될 테이블을 분류하는 방법이다. 이러한 분류 방법은 __테이블이 분할되지 않기 때문에 테이블간 독립성이 보장된 쿼리에서 높은 성능을 보장한다.__ 예를 들어, 저장 매체에 따른 분류로 텍스트, 사운드, 동영상과 같이 테이블을 분류하는 방법 또는 사용자 아이디별로 분류하여 테이블을 분류하는 방법 등이 있다.

+ 키 기반 분류  
__샤딩 시스템에서 가장 많이 사용하는 방법 중 하나로 테이블의 특정 필드를 기준으로 필드 값의 범위에 따라 샤드를 결정하는 방법이다.__ 보통 범위를 결정하는 필드를 샤드 키(shard key)라고 하며, 샤드 키의 분할 범위는 샤드 키의 전체 데이터 개수에 따라 자동으로 설정 또는 사용자가 설정하는 방법으로 나누어진다. 전자와 같이 자동으로 분할 범워를 결정하는 것을 자동 샤딩(auto sharding)이라고 하며, 후자와 같이 사용자가 설정하는 방법을 수동 샤딩(manual sharding)이라고 한다.

+ Look-up 테이블 기반 분류  
키 기반 분류 방법의 단점은 특정 영역으로 데이터 값이 몰릴 경우에 분할되기 전, 특정 샤드에 데이터가 집중되는 문제점을 가진다. 즉 데이터가 특정 샤드에 몰릴 경우에 해당 샤드는 부하가 발생되어 전체 시스템의 성능을 떨어뜨리는 결과를 초래한다. 또한 샤드가 한 쪽 범위에서 발생되기 때문에 잘못된 샤드 키 설정으로 인한 전체 시스템 성능이 개선되지 않는 문제점을 가지고 있다. 이러한 단점을 해결하기 위해 __해쉬 기법을 도입하여 샤드 키 자체를 분할하기 보다 입력된 샤드 키의 해쉬 값을 통한 분할 정책을 사용한다.__ 샤드 키의 해쉬 값은 랜덤 하게 균등 분배되어 특정 샤드 범위에 데이터가 몰리더라도 균등 분배되는 장점이 있다.

위에서 논한 세가지 샤드 전략이 각각 장단점을 모두 내포하기 때문에 어떤 전략을 사용하는가는 프로그래머에 의해 선택된다. 형태에 따른 분류는 독립된 쿼리가 보장되는 시스템에서 RDB를 분할하는 방법에 효과적이고, 키 기반 분류는 자동 샤딩 시스템에 잘 적용된다. 반면, __look-up 테이블은 scale-out 중심의 분산 데이터베이스에 적합한 구조이다.__

## Config 서버
MongoDB의 config 서버는 동일한 역할을 담당하고 있는 __3대로 구성된다.__ 3대로 구성되어 있다면 일반적으로 config 서버 집합은 fail-over를 위한 복제 구조를 사용한다고 판단할 수 있다. 즉, 3대 중 한 대가 master가 되어 master 서버의 데이터를 나머지 두 대의 slave가 데이터를 동기화시키는 구조로 생각된다. 또는 master-master 구조로 어느 서버에 데이터를 저장하든 나머지 두 대가 저장된 데이터를 공유할 수 있다고 생각한다.

하지만, MongoDB의 config 서버는 매우 독특하다. Config 서버 자체는 매우 수동적인 서버로 자기 스스로 데이터를 취득하기 위한 능동적인 역할을 하지 않는다. 그렇다면, 3대의 Config 서버들은 어떻게 데이터를 동기화시키는가? 방법은 간단하다. __Config 서버와 연결되어 있는 mongos를 통해 데이터를 동기화시킨다.__

![그림 4-4](./images/pic4-4.png)

[그림 4-4]는 mongos와 config 서버와의 연결을 보여준다. 우선 [그림 4-4]에서 mongos가 하나도 설치되지 않았다고 가정하면, config 서버 3개는 단지 서로들 간의 연결 고리가 없는 독립 서버 3대가 존재하는 형태가 된다. Mongos가 설치되면서 mongos가 사용할 config 서버 3대를 등록시키면, mongos는 config 서버와 연결을 시도한다. Config 서버와의 연결은 3대와 동시에 연결되며, 3대 중 한 대라도 연결이 되지 않을 경우는 mongos는 연결 실패로 MongoDB가 기동되지 않는다.

3대의 config 서버와 연결된 mongos는 연결 초기에 config 서버와 최초로 연결된 것을 확인되면 자신의 샤딩 정책을 포함한 메타 정보를 config 서버 3대에 동시에 보낸다.([그림 4-4]는 mongos와 config 서버와의 연결을 보여준다. 우선 [그림 4-4]에서 mongos가 하나도 설치되지 않았다고 가정하면, config 서버 3개는 단지 서로들 간의 연결 고리가 없는 독립 서버 3대가 존재하는 형태가 된다. Mongos가 설치되면서 mongos가 사용할 config 서버 3대를 등록시키면, mongos는 config 서버와 연결을 시도한다. Config 서버와의 연결은 3대와 동시에 연결되며, 3대 중 한 대라도 연결이 되지 않을 경우는 mongos는 연결 실패로 MongoDB가 기동되지 않는다.

3대의 config 서버와 연결된 mongos는 연결 초기에 config 서버와 최초로 연결된 것을 확인되면 자신의 샤딩 정책을 포함한 메타 정보를 config 서버 3대에 동시에 보낸다.[1] 이 부분을 유심히; 살펴볼 필요가 있다. 일반 복제 구조에서는 클라이언트는 master에게만 데이터를 보내고 master에 저장된 데이터를 slave 또는 다른 master 들이 동기화 매커니즘을 통해 데이터를 공유한다. 하지만, MongoDB는 자신이 가지고 있는 복제 매커니즘이 있음에도 불구하고 왜 mongos를 통해 config 서버에 데이터를 저장하는 것일까? 여러 가지 이유가 있을 수 있지만, 분명한 것은 config 서버는 복제 구조를 가지지 않는다는 것이며, 성능상의 이슈로 복제를 사용하지 않는 것으로 보인다.

성능상의 문제에 대해 조금 논의해 보자. 복제는 데이터 양이 많아서 응용이 이를 처리해 주기 힘든 경우에 알아서 데이터 동기화를 시키는 구조에 적합하다. 하지만, config 서버에 저장되는 데이터는 아주 극소수이다. 단지 샤드 메타 정보를 저장하기 위해 복제 구조로 config 서버를 만들 필요성을 느끼지 않은 것인가 판단된다. 또한 config 서버는 fail-over에 의한 master 선출도 없다. 복제 구조로 구성된 config 서버가 아니기 때문에 이는 당연하지만, mongos 입장에서는 config 서버의 중요성 때문에, 한 대라도 죽으면 정상적이지 않다고 판단한다.

이와 같이 mongos에서는 config 서버와의 통신을 매우 중요하게 생각한다. 따라서 mongos는 config와의 데이터 연산 중에서 쓰기 연산을 일관성이 강한 연산으로 처리한다. Mongos는 config 서버와 쓰기 연산을 config 서버에 확실하게 저장되어 있는지 응답을 확인하는 SAFE 모드로 동작한다.

그럼 ‘Mongos가 한대만 존재하는가?’라는 질문을 우리는 생각할 수 있다. MongoDB의 샤드 구조는 mongos 개수를 제한하지 않는다. [그림 4-4]에서 mongos #2가 새롭게 첨가된다고 가정하자. 우선, Mongos #2를 기동하기 위해서는 mongos #1과 동일하게 3대의 config 서버 정보를 넘겨주어야 한다. 이때 Mongos #2는 config 서버에 샤딩 메타 정보가 있는지 확인한다. 샤딩 정보를 확인한 mongos #2는 config 서버의 샤딩 정보를 읽어 들여 자신의 메모리에 데이터를 저장하고, mongos #1과 동일한 데이터를 공유한다. 그리고, mongos #1과 #2는 서로 연결하지 않고, config 서버를 통해 데이터를 공유한다.) 이 부분을 유심히 살펴볼 필요가 있다. 일반 복제 구조에서는 클라이언트는 master에게만 데이터를 보내고 master에 저장된 데이터를 slave 또는 다른 master 들이 동기화 매커니즘을 통해 데이터를 공유한다. 하지만, MongoDB는 자신이 가지고 있는 복제 매커니즘이 있음에도 불구하고 왜 mongos를 통해 config 서버에 데이터를 저장하는 것일까? 여러 가지 이유가 있을 수 있지만, 분명한 것은 config 서버는 복제 구조를 가지지 않는다는 것이며, 성능상의 이슈로 복제를 사용하지 않는 것으로 보인다.

성능상의 문제에 대해 조금 논의해 보자. 복제는 데이터 양이 많아서 응용이 이를 처리해 주기 힘든 경우에 알아서 데이터 동기화를 시키는 구조에 적합하다. 하지만, config 서버에 저장되는 데이터는 아주 극소수이다. 단지 샤드 메타 정보를 저장하기 위해 복제 구조로 config 서버를 만들 필요성을 느끼지 않은 것인가 판단된다. 또한 config 서버는 fail-over에 의한 master 선출도 없다. 복제 구조로 구성된 config 서버가 아니기 때문에 이는 당연하지만, mongos 입장에서는 config 서버의 중요성 때문에, 한 대라도 죽으면 정상적이지 않다고 판단한다.

이와 같이 mongos에서는 config 서버와의 통신을 매우 중요하게 생각한다. 따라서 mongos는 config와의 데이터 연산 중에서 쓰기 연산을 일관성이 강한 연산으로 처리한다. Mongos는 config 서버와 쓰기 연산을 config 서버에 확실하게 저장되어 있는지 응답을 확인하는 SAFE 모드로 동작한다.

그럼 ‘Mongos가 한대만 존재하는가?’라는 질문을 우리는 생각할 수 있다. MongoDB의 샤드 구조는 mongos 개수를 제한하지 않는다. [그림 4-4]에서 mongos #2가 새롭게 첨가된다고 가정하자. 우선, Mongos #2를 기동하기 위해서는 mongos #1과 동일하게 3대의 config 서버 정보를 넘겨주어야 한다. 이때 Mongos #2는 config 서버에 샤딩 메타 정보가 있는지 확인한다. 샤딩 정보를 확인한 mongos #2는 config 서버의 샤딩 정보를 읽어 들여 자신의 메모리에 데이터를 저장하고, mongos #1과 동일한 데이터를 공유한다. 그리고, mongos #1과 #2는 서로 연결하지 않고, config 서버를 통해 데이터를 공유한다.

다음은 config 서버에 저장되는 정보를 요약한 것이다.
+ 샤드 메타 정보
+ 분산 락
+ 복제 집합 정보


첫 번째 샤드 메타 정보는 mongos가 처리하는 청크(chunk) 단위의 샤딩 정보를 저장하고, 분산 락은 mongos들간의 전체 시스템 락을 의미한다. 이 부분에 대해서는 ‘MongoDB의 분산 락’ 절에서 자세히 다루도록 한다. 그리고 마지막으로 복제 집합 정보를 config 서버가 저장하는데, 이는 [그림 4-4]와 같이 새로운 mongos가 동일한 config 서버에 연결될 때, 자신이 관리하여야 할 또는 접속하여야 할 mongod 샤드 정보를 획득할 때, 해당 정보를 같이 얻을 수 있다.

## 라우터 - mongos
앞 절에서 살펴본 것과 같이, 중개자 역할을 담당하고 있는 mongos를 MongoDB에서는 라우터라고 말하고 있다. Mongos는 MongoDB의 샤딩 구조에서 가장 핵심이 되는 모듈이다. Mongod는 복제를 담당하고, mongos는 샤딩을 담당한다. 이와 같이 MongoDB는 역할에 따른 모듈을 분리함으로써 시스템에 몰리는 부하를 분산시킬 수 있는 구조로 개발되었다. 다음은 mongos가 수행하는 일들을 정리한 것이다.

+ Config 서버로부터 상태를 모니터링 하며, 샤딩 메타 정보를 취득하거나 변경된 부분을 저장한다.
+ 데이터 계층의 mongod의 데이터 크기를 계산하여 적절한 시기에 분할을 수행한다.
+ 데이터 계층의 복제 집합으로 구성된 한 샤드의 서버들을 모니터링 하여, master의 변경 정보를 취득한다.
+ 응용 계층의 질의를 분석하여 적절한 샤드에 질의를 수행한다.

### 라우터와 Config 서버
앞 절에서 논의한 바와 같이 mongos와 config 서버 간의 통신은 __mongos가 처리할 기본적인 데이터를 config 서버에 저장하는 구조이다. Mongos에 데이터를 저장하지 않는 것은, mongos 간의 연결이 없기 때문에, 나중에 첨가되는 mongos에 데이터 동기화 매커니즘을 위해 config 서버를 사용한다.__

mongos는 기본 10초 주기로 config 서버에 두 개의 질의를 수행한다. 첫 번째 질의는 mongos의 현 상태를 업데이트하는 것이고, 두 번째 질의는 분산 락에 대한 상태를 업데이트하는 것이다. Mongos에서 처리하는 분산 락은 balancer라는 명칭으로 mongos들 간의 데이터 공유를 수행 한다. 만약, 첫 번째 질의는 mongos가 샤딩을 수행한 상태라면 10초 주기에서 5초 주기로 config 서버와의 통신 주기를 짧게 가져간다.

### 라우터와 mongod
mongos는 샤드를 관리하기 위한 쓰레드 `ReplicaSetMonitorWatcher`와 `WriteBackListener`를 사용한다.  
`WriteBackListener` 쓰레드는 mongos에 샤드가 설정될 때, 샤드에 등록된 mongod 리스트를 취득하여 각각의 mongod와 연결을 유지시킨다.  
`ReplicaSetMonitorWatcher` 쓰레드는 10초 주기로 mongos가 관리하고 있는 mongod 서버에 복제 정보를 요청하여 데이터를 취득한다. 복제 정보에는 복제 집합 정보(이름과 노드 리스트)와 master 여부와 master 서버 주소, 그리고 mongod에 저장된 BSON 객체 크기를 알려준다.  

![그림 4-5](./images/pic4-5.png)

[그림 4-5]는 두 개의 mongos와 3개의 mongod로 구성되어진 모습이다. [그림 4-5]를 보면 mongos는 서로 다른 mongos와의 통신 연결선이 없는 대신에 모든 mongod 서버와 연결선을 가지고 있다. 즉, mongod 서버에 문제가 발생하면 mongos는 자기 스스로 문제점을 찾아 해결 방안을 강구하게 된다.
쓰레드 WriteBackListener는 최초 연결된 mongod와의 소켓 연결을 유지하여, mongod의 소켓이 close될 때, 이벤트를 발생시켜 mongod가 문제가 있음을 인지하고, config 서버로부터 변경된 샤드 정보가 있는지 요청한다. 획득한 config 서버의 정보를 이용하여 mongod 서버와 10초 주기로 재 접속을 시도한다. 그리고, 주기적인 복제 정보를 취득하는 쓰레드 ReplicaSetMonitorWatcher 역시 mongod의 상태를 체크하여 master 정보를 획득하고, master 정보가 획득되면 변경된 master로 연산을 수행한다. 자세히 살펴보면, 쓰레드 `WriteBackListener`의 역할은 한 샤드의 복제 정보가 변경되었을 경우에 샤드 정보를 업데이트하는 용도로 사용되고, 한 샤드를 구성하고 있는 복제 집합 노드의 fail 처리는 `ReplicaSetMonitorWatcher`에서 수행한다.

한가지 재미있는 사항은 mongos는 mongod의 문제점을 바로 인지할 수 있으나, 문제에 따른 master 취득과정이 실시간이 아니라는 것이다. 이러한 사항은 mongod의 복제 구조를 생각해 보면 쉽게 알 수 있다. Mongod의 복제 구조는 master가 fail이 되면 master를 선출하기 위해 투표를 수행한다. 이러한 투표 과정 시간이 약 10초, 길게는 30초~1분 정도 소요될 수 있기 때문에[1] mongos 입장에서는 master가 선출될 때까지 기다려야 한다. 운이 좋게도 master가 아닌 slave가 fail이라면 mongos 입장에서는 기존 master로 계속 연산을 수행한다.

### 라우터와 응용프로그램
응용은 직접 mongod로 데이터를 전송하는 것이 아니라, MongoDB의 라우터를 통해 데이터를 mongod에 저장한다. 따라서, 샤딩 구조에서의 MongoDB는 응용의 데이터 저장 구조를 3단계(tier)로 가지고 간다.

![그림 4-6](./images/pic4-6.png)

[그림 4-6]은 3단계로 구성된 mongos를 이용한 응용에서의 MongoDB를 사용하는 구조를 보여준다. 응용에서는 `sendToMongos()`라는 함수를 이용하여 mongos에 데이터를 전송한다. Mongos는 응용으로부터 전달 받은 데이터를 읽어 들여, 유효한 샤드를 검색하여 해당 샤드의 master로 `sendTomongod()` 함수를 통해 수행된다. 여기서 `sendToMongos()`와 `sendToMongod()` 함수의 차이점은 `sendToMongos()`는 mongos로부터 응답을 기다리고 있는 block mode 통신이고, `sendToMongod()`는 1장에서 살펴본 것과 같이 mongod의 소켓 버퍼에 데이터가 전달되었다는 것만 확인하고 리턴하는 함수이다. 즉, 응용과 mongos와의 통신은 일관성 SAFE와 유사하고, mongos와 mongod의 통신은 일관성 NORMAL과 동일하다.

한가지 주의할 점은 응용과 mongos의 통신이 일관성 SAFE와 유사하다고 해서, SAFE 모드로 동작한다는 것은 아니다. 여기서 유사라는 것은 SAFE 모드와 같이 저 수준의 소켓 라이브러리의 ACK가 아닌, 상대편의 응답을 기다리는 블록 모드라는 것이 동일한 개념이라는 것이다. 만약, 응용의 `sendToMongos()`가 일관성 SAFE 모드로 동작한다면, [그림 4-7]과 같이 동작한다.

![그림 4-7 샤딩 모드](./images/pic4-7.png)

MongoDB의 일관성 모드 SAFE는 데이터 저장소에 저장되었다는 것을 의미하므로, [그림 4-6]의 서버에 데이터가 도착하였다는 것보다, 데이터 저장 처리를 더 수행하여 mongod가 보내준 완료 메시지를 mongos가 받을 때까지 블록 모드로 대기한다. 샤딩 모드와 독립 모드에서의 MongoDB의 성능을 고려해 보자. 독립 모드에서의 NORMAL은 [그림 1-4](./images/pic1201.png)와 같은 구조로 동작되고, 샤딩 모드에서는 [그림 4-6]과 같이 동작한다. 그림의 차이에서 보듯이 샤딩 모드에서는 중간에 mongos가 동작하고, mongos와 응용은 블록 모드로 동작하는 시간이 독립 모드보다 더 첨가되었다. SAFE 일관성은 독립 모드에서의 동작은 [그림 1-5](./images/pic1202.png)와 같고, 샤딩 모드에서는 [그림 4-7]과 같다. 따라서, __샤딩 모드가 독립 모드보다 느리며, 실 성능 평가에서도 샤딩 모드가 3배 정도 느리게 나타난다.__

그렇다면, 왜 샤딩 모드로 MongoDB를 사용한다면 성능은 포기하여야 하는가? 하는 의문점이 생긴다. 앞에서 논한 3배 정도의 속도 저하는 독립 모드와 1:1:1로 구성된 한 개의 샤드 모드의 성능 평가에 따른 것이다. 속도가 느려지는 이유는 mongos의 데이터 처리 과정이 단순 데이터 전송이 아니라, 샤드를 선택하는 일련의 과정이 필요하기 때문이다. Mongos는 응용에서 전달된 샤드 키의 범위에 따라 특정 샤드를 선택하고, 데이터 균등 분배를 위한 밸런스 역할을 담당하는 mongos의 기능이 있다. 따라서, 샤딩 모드에서의 mongod의 성능은 독립 모드와 동일하다. 그렇다면 mongos와 mongod가 1:1의 관계를 가지지 않는다면, mongod의 성능을 최대로 구현할 수 있을 것이다. __그렇다. MongoDB는 mongos와 mongod의 비율을 꼭 1:1로 가져가지 않는다. Mongos와 응용과 mongod를 연결시켜주는 라우터로, 한 개의 mongod에 2개 이상의 mongos를 사용할 수 있다.__ [그림 4-8]은 mongod와 mongos의 관계를 1:n으로 구성한 예를 보여준다.

![그림 4-8](./images/pic4-8.png)


샤딩 모드가 독립 모드보다 약 3배 정도 느리기 때문에 [그림 4-8]과 같이 구성시킬 경우는 독립 모드에 준하는 성능까지 향상시킬 수 있다. 하지만, [그림 4-8]과 같이 구성할 이유는 전혀 없다. 이는 독립 모드로 구성시킨다면 응용을 포함하여 2대면 될 것을 구태여 4대 또는 7대로 구성할 필요가 없기 때문이다. [그림 4-8]에서 조금 더 나아가 보자. Mongos와 응용의 관계는 1:1로 유지되어야 하는가? 하는 의문도 생긴다. MongoDB는 응용이 mongos와의 개별적인 연결을 수행하기 때문에 꼭 1:1의 관계를 가질 필요는 없다. [그림 4-8]에서 응용을 3대가 아닌 1대로 [그림 4-9]와 같이 구성한다고 가정하자.[그림 4-9]의 응용의 성능 이슈가 있을 수 있지만, 샤딩 모드에서 부하는 mongos에서 이루어진다. 따라서, 응용의 성능에 따라 [그림 4-8]과 [그림 4-9]는 동일한 성능이 나타난다. 하지만, [그림 4-9]의 문제점이 없는 것은 아니다. 이 역시 이와 같이 사용할 경우는 __응용이 연결하고자 하는 mongos를 관리하고 있어야 하며, 특정 mongos가 fail 되었는지를 항상 모니터링하고 있어야 한다.__

![그림 4-9](./images/pic4-9.png)  

[그림 4-10]은 [그림 4-9]의 __mongos의 상태를 모니터링하는 문제점을 해결하기 위해 스위치에 가상 IP를 만들어 부하 분산을 시키고 있다.__ 이와 같을 경우, 응용은 가상 IP를 통해 한 개의 mongos만 연결을 시도한다. 만약 mongos #1이 fail 되었다면, 스위치는 자동으로 인식하여 해당 서버로 데이터를 보내지 않을 것이다. 또한 응용은 자신이 보낸 연산이 에러가 발생하였다면, 가상 IP로 재 연산을 시도하면 새로운 mongos로 연결되어 연산 처리를 계속 수행한다.상기와 같이 샤딩 구조에서 응용, mongos와 mongod와의 관계는 여러 가지 존재한다. 서비스를 구성할 때, 질의가 많은 시스템이라면, [그림 4-9]과 적합할 것이고, 쓰기 만을 수행하는 구조라면 경제성 있는 [그림 4-10]과 같이 적은 수의 응용으로 최적화 시킬 수 있을 것이다. 모든 분산 시스템과 마찬가지로 MongoDB 역시 모듈 별 구성 방법은 QC(품질 관리)를 통한 시스템 성격에 맞추어 가장 최적의 상태를 찾아 설계하는 것이 바람직하다.

![그림 4-10](./images/pic4-10.png)

## 분산 락
MongoDB의 분산 락 역시 일반 락과 동일하게, __mongos가 사용하게 되는 자원의 독점을 위해 분산 락을 사용한다.__ Mongos가 독점하는 자원은 다음과 같다.

+ 밸런서(balancer)의 연산
+ 컬렉션(collection)의 분할(split)
+ 컬렉션의 이관(migration)

상기의 내용을 보면 공유자원이기 보다는 연산 쪽에 가깝다. 맞다. Mongos는 mongos의 핵심 기능인 샤딩을 수행할 연산들에 대해 분산 락을 사용한다. 즉, 여러 개의 mongos가 설치되어 있다면, 설치된 mongos 중에 상기의 작업을 아무나 먼저 시작할 수 있고, 시작된 연산에 대해 독점하고 있음을 다른 mongos에 알려주어야 한다. 독점 여부를 알려주는 방법이 분산 락이다. [그림 4-11]은 mongos와 config 서버와의 분산 락 구조를 보여준다.

![그림 4-11](./images/pic4-11.png)

LockPinger 쓰레드의 중요한 역할은 30초 주기로 ping 타임을 갱신하는 것이다. 이 ping 타임은 분산 락 지속시간과 관련 있다. 예를 들어, 분산 락 지속시간이 일정 시간을 초과하였지만, ping 타임이 30초 주기로 계속 갱신을 하고 있다면, 시스템 부하가 발생한 것이 아니라, 분락 락을 이용한 연산이 오래 걸린다고 판단할 수 있다. 반대로, 분산 락 지속시간과 ping 타임이 동일하게 변하기 않고 있다면, 분산 락을 소유한 노드에 문제가 발생한 것으로 판단할 수 있다.

그럼, mongos가 분산 락을 어떻게 처리하는 지 흐름을 파악해 보자. [그림 4-11]과 같이 MongoDB는 분산 락을 특정 이름으로 구성한다. Mongos가 사용하는 분산 락의 이름은 balancer라는 고정된 분산 락과 컬렉션 명으로 구성된 분산 락 이름이 사용된다. Mongos에서 특정 이름의 분산 락을 확보하기 위해 lock_try()라는 함수를 호출한다. 함수 `lock_try()`는 primary(Mongos를 기동시킬 때, 지정된 첫 번째 config 서버가 primary 서버가 된다.)로 설정된 첫 번째 config 서버에 해당 이름의 락이 설정되어 있는지 요청한다. 왜 첫 번째 config 서버에서만 락 정보를 얻어오는가? MongoDB는 SyncCluster라는 개념의 연결 풀connection pool을 사용한다. SyncCluster는 n개의 서버와 동시에 연결되어 있으며, 쓰기 연산에 대해서는 n개의 서버에 동시에 데이터를 전달하고, 읽기 연산에 대해서는 n개 중 한 개의 응답만 전달되면 값을 리턴해 준다. Config 서버는 3대 모두 항상 죽어서는 안 되는 구조이기 때문에 읽기 연산인 분산 락 정보를 취득하는 것은 첫 번째 서버에서만 얻어오게 된다.

mongos는 취득한 분산 락 정보에 대한 결과를 분석한다. 우선, 락 정보가 존재하지 않다면, 다른 mongos에서도 한번도 획득한 적이 없는 최초의 분산 락이므로, 락 정보를 생성하고 분산 락을 취득했음을 config 서버에 통보한다. Config 서버로부터 받아온 분산 락 정보가 있다면, mongos는 분산 락 상태 플래그를 조사한다. 분산 락 상태 플래그는 0 값을 가질 경우, unlock 되었다는 것을 의미하고, 0이 아닌 값은 누군가에 의해 해당 분산 락이 lock 되었음을 의미한다. 해당 분산 락의 취득 시간과 ping 타임을 비교하여 15분을 초과하였을 경우는 분산 락을 취득한 시스템에 문제가 발생하였다고 판단하고 락을 강제로 해제하고 새로운 소유자가 소유되었음을 통보한다. 분산 락의 활용이 완료되었다면, mongos는 `unlock()` 함수를 통해 해당 분산 락의 상태 플래그를 0으로 변환하고, 분산 락이 해제되었음을 config 서버에 통보한다.

이 부분에서 우리는 한가지 의문점이 생긴다. 즉, 두 개의 mongos에서 동시에 동일한 분산 락을 획득하기 위해 시도할 경우에는 어떻게 될 것인가? 다행스럽게도 MongoDB는 앞 절에서 살펴보았듯이 쓰기 연산은 락이 전역적으로 한 개만 존재하며, mongos는 config 서버와의 쓰기 모드를 SAFE로 처리하기 때문에 쓰기에 관한 트랜젝션(transaction)은 보호된다. 다만, [그림 4-11]에서 락을 취득하기 위해서는 락 요청과 락 취득 이라는 2단계 작업이 수행되는데, 락 요청과 취득 사이에 다른 mongos에서 분산 락을 취득할 수 있다. 이러한 문제를 위해 mongos는 분산 락 취득 후, 다시 분산 락 정보는 요청하여 소유자가 자신과 동일한지 판단하고, 동일하다면 성공 아니면 실패로 간주한다.

## 샤딩의 한계

+ 한 청크에 저장될 수 있는 BSON 객체의 개수는 250,000개이다.
+ 한 청크에 설정할 수 있는 분할 지점의 최대 개수는 8,192개이다.
+ MongoDB로 설정할 수 있는 샤드 노드의 개수는 1,000개가 목표이다. 하지만, 10gen에서도 테스트된 샤드의 개수를 100개 정도로 한정하고 있다.

## 데이터 유실 가능성
앞 장에서도 이 문제에 대해서도 언급하였지만, 샤딩 구조에서의 데이터 유실 가능성에 대해서도 논의할 필요가 있다. 샤딩 구조가 아닌 복제 구조에서는 master가 fail되고 난 뒤 선출하게 되는 약 10초 동안의 데이터가 유실될 수 있다고 하였다. 이러한 문제는 샤딩 구조에서도 동일하게 발견된다. 샤딩 구조가 아닌 시스템에서는 mongos와 같은 라우터가 존재하지 않기 때문에 응용은 항상 master만을 바라보게 된다. 따라서, master의 fail을 바로 알게 되고, 프로그래머는 master fail에 대한 대처를 직접 작성하여야 한다.

![그림 4-12](./images/pic4-12.png)

문제는 샤딩 구조에서 여러 가지 문제점들을 보일 수 있다. [그림 4-12]와 같이 3개의 mongod와 1개의 mongos가 연결되었다고 가정하자. 그림과 같이 mongod #1이 master로 설정되어 있다. 정상적으로 잘 동작하고 있던 MongoDB에 부하가 발생하고 있고, 원인을 알 수 없는 문제로 인해 master 서버가 fail 되었다. [그림 4-13]과 같이 mongos는 쓰레드 `ReplicaSetMonitorWatcher`가 수행되기 전까지 아직도 mongod #1을 master로 간주한다. 이 때 응용으로부터 전달된 질의를 수행하면 master와의 연결이 끊어진 경우이기 때문에 mongos는 에러를 발생한다.

![그림 4-13](./images/pic4-13.png)

일정 시간이 지나, 쓰레드 `ReplicaSetMonitorWatcher`이 mongod #1이 fail된 것을 인식하고, 복제 정보를 갱신한다. [그림 4-14]과 같이 MongoDB는 투표를 통해 mongod #2를 master로 선출하였고, mongos 역시 mongod #2를 master로 인식하여 정상적인 작업을 수행하게 된다.

![그림 4-14](./images/pic4-14.png)

이 시점을 가만히 고려해 보자. [그림 4-9]에서 정상적으로 입력된 데이터가 아직 복제를 성공하지 못하고, master가 fail이 되었다면, 새롭게 선출된 master에 있는 oplog 데이터로 새로운 복제를 수행하게 될 것이다. 이와 같을 경우 mongos는 mongod #1에 삽입하였던 데이터가 사라졌다는 것을 인식할 수 없게 된다. 앞 장에서 살펴보았던 10초의 룰이 적용되었기 때문에, __master 선출에서 최악의 경우 10초의 데이터가 사라질 수 있다.__

또한, mongos는 master 선출에 관여할 수 없기 때문에 master가 없다면, slave가 읽기 연산 모드를 지원해 주지 않는다면, MongoDB 자체는 전체 다운이 되어 버린다. 예를 들어, [그림 4-9]의 환경에서 mongod #1이 먼저 fail되지 않는다면, mongod #1이 master 자격을 유지하고 있기 때문에 나머지 두 대의 slave가 fail이 되어도 1대의 master로 시스템은 유지된다. 하지만, [그림 4-11]과 같이 새로운 master가 선출되고 난 다음에, 새롭게 선출된 master인 mongod #2가 fail된다면, salve인 mongod #3는 투표를 수행할 수 없게 된다. 따라서, slave만 존재하는 상태가 되어, 정상적인 MongoDB 수행이 어려워진다.
